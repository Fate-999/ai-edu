## 最小二乘法（Method of least squares）

### 算法原理

有了样本数据后，我们通过可视化可以看到这个问题应该可以通过线性回归的问题来解决。如果在高维空间没有可视化手段时，一般也是先用简单的线性回归来尝试解决，如果效果不令人满意的话，再尝试非线性的模型。

在图1.3中，我们发现有很多条直线可以穿过整个样本区来拟合，它们的 $a, b$ 值各不相同，哪一条是最佳的呢？

德国科学家卡尔·高斯（Karl Gauss，1777-1855）提出用图1.4中竖直方向的距离 error 的平方和来估计参数 $a$ 和 $b$，该方法称为最小二乘法或最小平方法。

<img src="./images/1-4.png" />

点到直线的距离本来应该用垂直距离来计算，但是那样的话计算量太大了，竖直距离用一个减法就可以得到，而且和垂直距离是正相关的，所以完全可以代替垂直距离。

用最小二乘法拟合的直线具有一些优良的性质$^{[1]}$：

1. 该方法得到的回归直线能使离差平方和最小，虽然这并不能保证它就是拟合数据的最佳直线，但也是足够好的。
2. 该方法可以得知 $a$ 和 $b$ 的估计量的抽样分布。
3. 在某些条件下，与其他方法相比，其抽样分布具有较小的标准差。

在图1.4中，用虚线表示的用于拟合的直线方程是：

$$
\hat y = \hat a x + \hat b \tag{1.3}
$$

其中，$\hat a$ 和 $\hat b$（读作a hat，b hat），表示是通过算法估计出来的 $a$ 值和 $b$ 值，而不是原始值（$a=0.5$，$b=1$）；$\hat y$ 表示是估算出来的结果值，而不是原始的标签值 $y$。

图1.4中的 error 表示每个样本点与估算值的误差：

$$
error = y - \hat{y}=y -  (\hat a x + \hat b) \tag{1.4}
$$

这些误差有正有负，我们不希望在做加法时正负抵消，所以用误差的平方再相加，
令$J=\sum\limits_{i=1}^n error^2:$

$$
J = \sum_{i=1}^n (y_i -  (\hat a x_i + \hat b))^2 \tag{1.5}
$$

公式 1.5 被称为均方差，是一个凸函数，根据微分极值定理，对 $J$ 分别求 $a$ 和 $b$ 偏导，然后令导数公式为 0，就可以得到 $\hat a$ 和 $\hat b$，使得 $J$ 的值最小，即总体误差最小。

$$
\begin{cases}

\frac{\partial J}{\partial a} = -2 \sum\limits_{i=1}^n(y_i - \hat a x_i - \hat b)x_i =0
\\\\
\frac{\partial J}{\partial b} = -2\sum\limits_{i=1}^n(y_i - \hat a x_i - \hat b) =0\\

\end{cases}
\tag{1.6}
$$

先从 1.6 的第二个方程整理后得到：

$$
\hat b = \frac{1}{n} \sum_{i=1}^n (y_i - \hat a x_i) \tag{1.7}
$$


把 1.6 第一个公式展开：

$$
\sum_{i=1}^nx_iy_i-\hat{a}\sum_{i=1}^nx_i^2-\hat{b}\sum_{i=1}^nx_i=0 \tag{1.8}
$$

再把 1.7 带入 1.8：

$$
\sum_{i=1}^nx_iy_i-\hat{a}\sum_{i=1}^nx_i^2- \frac{1}{n} \sum_{i=1}^n (y_i - \hat a x_i) \sum_{i=1}^nx_i=0 \tag{1.9}
$$

等式两边乘以 $n$，并展开：

$$
n\sum_{i=1}^nx_iy_i-n\hat{a}\sum_{i=1}^nx_i^2 - \sum_{i=1}^nx_i\sum_{i=1}^n y_i + \hat{a}(\sum_{i=1}^nx_i)^2=0
\tag{1.10}
$$

把包含 $\hat{a}$ 的两项移到一侧，并提取出来：

$$
\hat a = \frac{n\sum\limits_{i=1}^n x_i y_i - \sum\limits_{i=1}^n x_i \sum\limits_{i=1}^n y_i}{n\sum\limits_{i=1}^nx_i^2 - (\sum\limits_{i=1}^nx_i)^2} \tag{1.11}
$$

### 代码实现

下面用代码实现一下，首先计算 $\hat a$ 的值：

```Python
# 根据公式1.11
def calculate_a(X,Y):
    n = X.shape[0]
    numerator = n * np.sum(X*Y) - np.sum(X) * np.sum(Y)
    denominator = n * np.sum(X*X) - np.sum(X) * np.sum(X)
    a = numerator / denominator
    return a
```

其中 np.sum(X\*Y)，相当于公式中的 $\sum\limits_{i=1}^n x_i y_i$，由于 $\bold X$ 和 $\bold Y$ 都是（100x1）的数组，所以 X\*Y 会按元素相乘，得到一个新的（100x1）的数组，然后求和。

再计算 $\hat b$ 的值：
```Python
# 根据公式1.7
def calculate_b(a,X,Y):
    n = X.shape[0]
    b = (np.sum(Y - a * X))/n
    return b
```

最后得到的结果是：

```
a_hat=0.4979, b_hat=1.0061
```
把拟合结果变成直线方程，即 $y=0.4979x+1.0061$，绘制在原始的样本图上，得到图1.5。

<img src="./images/1-5.png" />

还记得我们在制作数据集时设置的参数值吗？$a=0.5$，$b=1$，由于在制作样本数据集时有 $\varepsilon$ 的存在，所以最后估计出来的结果 $\hat a=0.4979...$，$\hat b=1.0061...$，非常接近真实值。
