## 1.3 最小二乘法（Ordinary Least Squares）

### 算法原理

有了样本数据后，我们通过可视化可以看到这个问题应该可以通过线性回归的问题来解决。如果在高维空间没有可视化手段时，一般也是先用简单的线性回归来尝试解决，如果效果不令人满意的话，再尝试非线性的模型。

在图 1.1.2 中，我们发现有很多条直线可以穿过整个样本区来拟合，它们的 $a、b$ 值各不相同，哪一条是最佳的呢？即问题转变为如何评估拟合直线的误差问题。

德国科学家卡尔·高斯（Karl Gauss，1777-1855）提出用图 1.3.1 中竖直方向的距离 error 的平方和作为误差来估计参数 $a$ 和 $b$，即 1.2 小节中提到的均方差；而 1806 年，法国科学家勒让德发明了最小二乘法，或最小平方法；1829年，高斯提供了最小二乘法的优化效果强于其他方法的证明。

<img src="./images/1-3-1.png" />
<center>图 1.3.1 误差的计算方法</center>

点到直线的距离本来应该用垂直距离来计算，但是那样的话计算量太大了，竖直距离用一个减法就可以得到，而且和垂直距离是正相关的，所以完全可以代替垂直距离。

用最小二乘法拟合的直线具有一些优良的性质$^{[1]}$：

1. 该方法得到的回归直线能使离差平方和最小，虽然这并不能保证它就是拟合数据的最佳直线，但也是足够好的。
2. 该方法可以得知 $a$ 和 $b$ 的估计量的抽样分布。
3. 在某些条件下，与其他方法相比，其抽样分布具有较小的标准差。

在图 1.3.1 中，用虚线表示的用于拟合的直线方程是：

$$
\hat y = \hat a x + \hat b \tag{1.3.1}
$$

其中，$\hat a$ 和 $\hat b$（读作a hat，b hat），表示是通过算法估计出来的 $a$ 值和 $b$ 值，而不是原始值（$a=0.5$，$b=1$）；$\hat y$（读作y hat）表示是估算出来的结果值，而不是原始的标签值 $y$。

图 1.3.1 中的 error 表示每个样本点的标签值与估算值的误差：

$$
error = y - \hat{y}=y -  (\hat a x + \hat b) \tag{1.3.2}
$$

这些误差有正有负，我们不希望在做加法时正负抵消，所以用误差的平方再相加，
令$J=\sum\limits_{i=1}^n error_i^2:$

$$
J = \sum_{i=1}^n [y_i -  (\hat a x_i + \hat b)]^2 \tag{1.3.3}
$$

公式 1.3.3 被称为均方差，是一个凸函数。根据微分极值定理，对 $J$ 分别求 $a$ 和 $b$ 偏导，然后令导数公式为 0，就可以得到最佳的 $\hat a$ 和 $\hat b$，使得 $J$ 的值最小，即总体误差最小。

$$
\begin{cases}
\frac{\partial J}{\partial a} = -2 \sum\limits_{i=1}^n(y_i - \hat a x_i - \hat b)x_i =0
\\\\
\frac{\partial J}{\partial b} = -2\sum\limits_{i=1}^n(y_i - \hat a x_i - \hat b) =0
\end{cases}
\tag{1.3.4}
$$

解得：

$$
\begin{cases}
\hat b = \frac{1}{n} \sum\limits_{i=1}^n (y_i - \hat a x_i)
\\\\
\hat a = \frac{n\sum\limits_{i=1}^n x_i y_i - \sum\limits_{i=1}^n x_i \sum\limits_{i=1}^n y_i}{n\sum\limits_{i=1}^nx_i^2 - (\sum\limits_{i=1}^nx_i)^2}
\end{cases}
\tag{1.3.5}
$$

公式推导详见 1.5 小节。


### 代码实现

下面用代码实现一下。由于 $\hat{b}$ 依赖于 $\hat{a}$，所以我们先计算 $\hat{a}$：

```Python
# 公式 1.3.5
def least_square_1(X,Y):
    n = X.shape[0]
    # a_hat
    numerator = n * np.sum(X*Y) - np.sum(X) * np.sum(Y)
    denominator = n * np.sum(X*X) - np.sum(X) * np.sum(X)
    a_hat = numerator / denominator
    # b_hat
    b_hat = (np.sum(Y - a_hat * X))/n
    return a_hat, b_hat
```

其中 np.sum(X\*Y)，相当于公式中的 $\sum\limits_{i=1}^n x_i y_i$，由于 $\boldsymbol{X}$ 和 $\boldsymbol{Y}$ 都是（200x1）的数组，所以 $\boldsymbol{X}*\boldsymbol{Y}$ 会按元素相乘，得到一个新的（200x1）的数组，然后求和。

最后得到的结果是：

```
a_hat=0.5066, b_hat=0.9921
```
把拟合结果变成直线方程，即 $y=0.5066x+0.9921$，绘制在原始的样本图上，得到图1.3.2。

<img src="./images/1-3-2.png" />
<center>图 1.3.2 拟合结果</center>

还记得我们在制作数据集时设置的参数值吗？$a=0.5$，$b=1$，由于在制作样本数据集时有 $\varepsilon$ 的存在，所以最后估计出来的结果 $\hat a=0.5066...$，$\hat b=0.9921...$，非常接近真实值。

### 计算均方差

可能会有人有疑问，为什么没有得到 $a=0.5、b=1$ 的解，而是得到了一个近似解？那我们就来看一下它们的均方差分别是多少吧。

```Python
# 计算均方差
def calculate_mse(Y, Y_hat):
    return np.sum((Y-Y_hat)*(Y-Y_hat))/Y.shape[0]

# 比较估计值和原始的均方差的大小
def compare(a_hat,b_hat,a,b,X,Y):
    Y_hat1 = a_hat * X + b_hat
    mse1 = calculate_mse(Y, Y_hat1)
    Y_hat2 = a * X + b
    mse2 = calculate_mse(Y, Y_hat2)
    return mse1, mse2

mse1, mse2 = compare(a_hat, b_hat, 0.5, 1, X, Y)
print(str.format("mse1={0:.6f}, mse2={1:.6f}", mse1, mse2))
```
先用最小二乘法得到的 $\hat{a}、\hat{b}$，把样本数据的 $x$ 值代入回归方程 $y=\hat{a}x+\hat{b}$ 来计算出对应的 $\hat{y}_1$，然后用样本标签数据 $y$ 来做基准计算均方差，得到 mse1；然后再用 $a=0.5、b=1$ 计算出 mse2，结果为：

```
mse1=0.002558, mse2=0.002580
```

可以看到 mse1 < mse2，说明最小二乘法估算出的参数值更准确一些。这是由于原始数据中加入了噪音（方差、偏差），使得预设的参数值已经偏离了 $a=0.5、b=1$。

