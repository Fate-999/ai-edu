## 2.1 多元线性回归

### 线性回归分类

有了前面学习的基本概念之后，我们进一步认识一下线性回归问题。在统计学中，线性回归分为三类：

- 简单线性回归：一个自变量，一个因变量。
- 多重线性回归：多个自变量，一个因变量。
- 多元线性回归：多个自变量，多个因变量。

在机器学习中，我们把第 2 个称作**多元线性回归**，把第 3 个称作**线性分类**或**标注**。

机器学习领域中，多元线性回归的一般形式为公式 2.2.2 所示：

$$
y = a_1x_1 + a_2x_2 +a_3x_3 + \dots + a_kx_k + b \tag{2.1.1}
$$

其中：
- $x_i$ 表示一个样本数据的第 $i$ 个特征值，自变量。
- $y$ 是因变量，表示回归的结果。
- $a_i$ 表示模型的第 $i$ 个参数。
- $b$ 仍然是偏移量，也是模型参数。

### 线性回归的应用条件

第一章中学习了在二维平面上，解决一元线性回归问题的最小二乘法，那么这个方法是否可以扩展到三维空间上甚至高维空间呢？这就是多元线性回归问题。

在二维平面上，用可视化的方式可以很容易地判别一个问题是否可以用线性回归模型来解决，在高维空间上就不那么容易了。所以我们先来学习一下线性回归的应用场景。

<这里有一张图>

1. 线性回归是一个回归问题（regression）。
   
    与回归相对的是分类问题（classification），分类问题要预测的变量 $y$ 输出集合是有限的，预测值只能是有限集合内的一个。当要预测的变量 $y$ 输出集合是无限且连续，我们称之为回归。比如，天气预报预测明天是否下雨，是一个二分类问题；预测明天的降雨量多少，就是一个回归问题。

2. 要预测的变量y与自变量x的关系是线性的。

    线性通常是指变量之间保持等比例的关系，从图形上来看，变量之间的形状为直线，斜率是常数。这是一个非常强的假设，数据点的分布呈现复杂的曲线，则不能使用线性回归来建模。

3. 各项误差服从正态分布，均值为0，与x同方差。

    前面最小二乘法求解过程已经提到了误差的概念，误差可以表示为误差 = 实际值 - 预测值。

    可以这样理解这个假设：线性回归允许预测值与真实值之间存在误差，随着数据量的增多，这些数据的误差平均值为0；从图形上来看，各个真实值可能在直线上方，也可能在直线下方，当数据足够多时，各个数据上上下下相互抵消。如果误差不服从均值为零的正太分布，那么很有可能是出现了一些异常值，数据的分布很可能是安斯库姆四重奏右下角的情况。

    这也是一个非常强的假设，如果要使用线性回归模型，那么必须假设数据的误差均值为零的正态分布。

4. 变量 x 的分布要有变异性。

    线性回归对变量 x也有要求，要有一定变化，不能像安斯库姆四重奏右下角的数据那样，绝大多数数据都分布在一条竖线上。

5. 多元线性回归中不同特征之间应该相互独立，避免线性相关。

    如果不同特征不是相互独立，那么可能导致特征间产生共线性，进而导致模型不准确。举一个比较极端的例子，预测房价时使用多个特征：房间数量，房间数量*2，-房间数量等，特征之间是线性相关的，如果模型只有这些特征，缺少其他有效特征，虽然可以训练出一个模型，但是模型不准确，预测性差。
