## 正规方程法

### 最小二乘法的矩阵解法

最小二乘法在多元线性回归中仍然适用，比如，我们还是使用均方差作为误差函数，则有：

$$
J = \sum_{i=1}^n [y_i - (a_1 x_{i,1} + a_2 x_{i,2}+b)]^2 \tag{2.2.1}
$$


然后分别求 $J$ 对 $a_1, a_2, b$ 的偏导数并令其结果为 0，会得到一个三元一次的方程组，通过解方程组即可得到 $a_1, a_2, b$ 的值。

但是，实际上多元线性回归的一般形式为公式 2.2.2 所示：

$$
y = a_1x_1 + a_2x_2 +a_3x_3 + \dots + a_kx_k + b \tag{2.2.2}
$$

如果分别对 $a_1 \dots a_k$ 求偏导，再加上 $b$，那就要解一个 $k+1$ 元一次方程组，这种方法就会变得很麻烦。所以，我们下面要寻求一种普遍适用的方法。

本小节中我们学习最小二乘的矩阵解法。矩阵法比代数法要简洁，且矩阵运算可以取代循环，所以现在很多机器学习库都是用的矩阵法来做最小二乘法。

根据公式 2.2.1，假设我们只有 4 个样本数据，并把 $b$ 改名为 $a_0$，其含义不变：

$$
y_1 = a_0+a_1 x_{1,1}+a_2 x_{1,2}
\\\\
y_2 = a_0+a_1 x_{2,1}+a_2 x_{2,2}
\\\\
y_3 = a_0+a_1 x_{3,1}+a_2 x_{3,2}
\\\\
y_4 = a_0+a_1 x_{4,1}+a_2 x_{4,2}
\tag{2.2.2}
$$

写成矩阵形式：

$$
\boldsymbol{Y} = 
\begin{pmatrix}
y_1
\\\\
y_2
\\\\
y_3
\\\\
y_4
\end{pmatrix}
=a_0
\begin{pmatrix}
    1
    \\\\
    1
    \\\\
    1
    \\\\
    1
\end{pmatrix}
+a_1 
\begin{pmatrix}
    x_{1,1} 
    \\\\
    x_{2,1}
    \\\\
    x_{3,1}
    \\\\
    x_{4,1}
\end{pmatrix}+a_2 
\begin{pmatrix}
    x_{1,2} 
    \\\\
    x_{2,2}
    \\\\
    x_{3,2}
    \\\\
    x_{4,2}
\end{pmatrix}
\tag{2.2.3}
$$


把 $(1,1,1,1)^T$ 和两列 $x$ 合并：

$$\boldsymbol{Y} =
\begin{pmatrix} 
1 & x_{1,1} & x_{1,2}
\\\\
1 & x_{2,1} & x_{2,2}
\\\\
1 & x_{3,1} & x_{3,2}
\\\\
1 & x_{4,1} & x_{4,2}
\end{pmatrix} 
\begin{pmatrix} 
a_0 \\\\ a_1 \\\\ a_2
\end{pmatrix} 
\tag{2.2.4}
$$

最后都写成矩阵形式：

$$
\boldsymbol{Y} = \boldsymbol{X}\boldsymbol{A} \tag{2.2.5}
$$

当$\boldsymbol{X}^T\boldsymbol{X}$为满秩或正定矩阵时，有：

$$
\boldsymbol{A} = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{Y} \tag{2.2.6}
$$

公式推导详见 2.3 小节。

假设原始的样本数据 $\boldsymbol{X_0} \in \mathbb{R}^{4 \times 2}$，左侧增加一列 1 之后 $\boldsymbol{X} \in \mathbb{R}^{4 \times 3}$， $\boldsymbol{Y} \in \mathbb{R}^{4 \times 1}$，则公式 2.2.6 的结果  $\boldsymbol{A} \in \mathbb{R}^{3 \times 1}$，其中 $b$ 在 $\boldsymbol{A}[0,0]$ 位置，$a_1$ 在 $\boldsymbol{A}[0,1]$ 位置，$a_2$ 在 $\boldsymbol{A}[0,2]$ 位置。

公式 2.2.6 还可以这样得到：

1. 从公式 2.2.1 得到损失函数的矩阵写法：

$$
J = \frac{1}{2} (\boldsymbol{Y} - \boldsymbol{X} \hat{\boldsymbol{A}})^T (\boldsymbol{Y} - \boldsymbol{X} \hat{\boldsymbol{A}}) \tag{2.2.7}
$$

2. 对 $J$ 求向量 $\boldsymbol{A}$ 的偏导，并令其为 0：

$$
\frac{\partial J}{\partial A} = \boldsymbol{X}^T(\boldsymbol{X}\boldsymbol{A}-\boldsymbol{Y}) = 0 \tag{2.2.8}
$$

3. 展开公式 2.2.8 即可得到公式 2.2.6。

公式推导详见 2.3 小节。

代码实现：
```Python
def normal_equation(X,Y):
    num_example = X.shape[0]
    # 在原始的X矩阵最左侧加一列1
    ones = np.ones((num_example,1))
    x = np.column_stack((ones, X))    
    # X^T * X
    p = np.dot(x.T, x)
    # (X^T * X)^{-1}
    q = np.linalg.inv(p)
    # (X^T * X)^{-1} * X^T
    r = np.dot(q, x.T)
    # (X^T * X)^{-1} * X^T * Y
    A = np.dot(r, Y)
    # 按顺序
    b = A[0,0]
    a1 = A[1,0]
    a2 = A[2,0]
    return a1, a2, b
```

我们用正规方程法来解决 1.3 中的一元线性回归问题也是可以的，区别是只返回 $a、b$ 两个值。代码如下：

```Python
if __name__ == '__main__':
    file_name = "1-0-data.csv"
    samples = load_file(file_name)
    if samples is not None:
        X = samples[:, 0].reshape(200,1)
        Y = samples[:, 1].reshape(200,1)
        a, b = normal_equation(X,Y)
        print(str.format("a={0:.4f}, b={1:.4f}", a,b))
    else:
        print("cannot find file " + file_name)
```
得到结果：
```
a=0.5066, b=0.9921
```
结果和最小二乘的代数解法一致。
