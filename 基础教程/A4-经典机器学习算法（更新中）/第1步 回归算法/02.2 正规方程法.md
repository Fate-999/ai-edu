## 正规方程法

### 最小二乘法的矩阵解法

最小二乘法在多元线性回归中仍然适用，比如，我们还是使用均方差作为误差函数，则有：

$$
J = \sum_{i=1}^n [y_i - (a_1 x_{i,1} + a_2 x_{i,2}+b)]^2 \tag{2.2.1}
$$


然后分别求 $J$ 对 $a_1, a_2, b$ 的偏导数并令其结果为 0，会得到一个三元一次的方程组，通过解方程组即可得到 $a_1, a_2, b$ 的值。

但是，实际上多元线性回归的一般形式为公式 2.2.2 所示：

$$
y = a_1x_1 + a_2x_2 +a_3x_3 + \dots + a_kx_k + b \tag{2.2.2}
$$

如果分别对 $a_1 \dots a_k$ 求偏导，再加上 $b$，那就要解一个 $k+1$ 元一次方程组，这种方法就会变得很麻烦。所以，我们下面要寻求一种普遍适用的方法。

本小节中我们学习最小二乘的矩阵解法。矩阵法比代数法要简洁，且矩阵运算可以取代循环，所以现在很多机器学习库都是用的矩阵法来做最小二乘法。

根据公式 2.2.1，假设我们只有 4 个样本数据，并把 $b$ 改名为 $a_0$，其含义不变：

$$
y_1 = a_0+a_1 x_{1,1}+a_2 x_{1,2}
\\\\
y_2 = a_0+a_1 x_{2,1}+a_2 x_{2,2}
\\\\
y_3 = a_0+a_1 x_{3,1}+a_2 x_{3,2}
\\\\
y_4 = a_0+a_1 x_{4,1}+a_2 x_{4,2}
\tag{2.2.2}
$$

写成矩阵形式：

$$
\boldsymbol{Y} = 
\begin{pmatrix}
y_1
\\\\
y_2
\\\\
y_3
\\\\
y_4
\end{pmatrix}
=a_0
\begin{pmatrix}
    1
    \\\\
    1
    \\\\
    1
    \\\\
    1
\end{pmatrix}
+a_1 
\begin{pmatrix}
    x_{1,1} 
    \\\\
    x_{2,1}
    \\\\
    x_{3,1}
    \\\\
    x_{4,1}
\end{pmatrix}+a_2 
\begin{pmatrix}
    x_{1,2} 
    \\\\
    x_{2,2}
    \\\\
    x_{3,2}
    \\\\
    x_{4,2}
\end{pmatrix}
\tag{2.2.3}
$$


把 $(1,1,1,1)^T$ 和两列 $x$ 合并：

$$\boldsymbol{Y} =
\begin{pmatrix} 
1 & x_{1,1} & x_{1,2}
\\\\
1 & x_{2,1} & x_{2,2}
\\\\
1 & x_{3,1} & x_{3,2}
\\\\
1 & x_{4,1} & x_{4,2}
\end{pmatrix} 
\begin{pmatrix} 
a_0 \\\\ a_1 \\\\ a_2
\end{pmatrix} 
\tag{2.2.4}
$$

最后都写成矩阵形式：

$$
\boldsymbol{Y} = \boldsymbol{X}\boldsymbol{A} \tag{2.2.5}
$$

当$\boldsymbol{X}^T\boldsymbol{X}$为满秩或正定矩阵时，有：

$$
\boldsymbol{A} = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{Y} \tag{2.2.6}
$$

公式推导详见 2.3 小节。

假设原始的样本数据 $\boldsymbol{X_0} \in \mathbb{R}^{4 \times 2}$，左侧增加一列 1 之后 $\boldsymbol{X} \in \mathbb{R}^{4 \times 3}$， $\boldsymbol{Y} \in \mathbb{R}^{4 \times 1}$，则公式 2.2.6 的结果  $\boldsymbol{A} \in \mathbb{R}^{3 \times 1}$，其中 $b$ 在 $\boldsymbol{A}[0,0]$ 位置，$a_1$ 在 $\boldsymbol{A}[0,1]$ 位置，$a_2$ 在 $\boldsymbol{A}[0,2]$ 位置。

公式 2.2.6 还可以这样得到：

1. 从公式 2.2.1 得到损失函数的矩阵写法：

$$
J = \frac{1}{2} (\boldsymbol{Y} - \boldsymbol{X} \hat{\boldsymbol{A}})^T (\boldsymbol{Y} - \boldsymbol{X} \hat{\boldsymbol{A}}) \tag{2.2.7}
$$

2. 对 $J$ 求向量 $\boldsymbol{A}$ 的偏导，并令其为 0：

$$
\frac{\partial J}{\partial A} = \boldsymbol{X}^T(\boldsymbol{X}\boldsymbol{A}-\boldsymbol{Y}) = 0 \tag{2.2.8}
$$

3. 展开公式 2.2.8 即可得到公式 2.2.6。

公式推导详见 2.3 小节。

代码实现：
```Python
def normal_equation(X,Y):
    num_example = X.shape[0]
    # 在原始的X矩阵最左侧加一列1
    ones = np.ones((num_example,1))
    x = np.column_stack((ones, X))    
    # X^T * X
    p = np.dot(x.T, x)
    # (X^T * X)^{-1}
    q = np.linalg.inv(p)
    # (X^T * X)^{-1} * X^T
    r = np.dot(q, x.T)
    # (X^T * X)^{-1} * X^T * Y
    A = np.dot(r, Y)
    # 按顺序
    b = A[0,0]
    a1 = A[1,0]
    a2 = A[2,0]
    return a1, a2, b
```

我们用正规方程法来解决 1.3 中的一元线性回归问题也是可以的，区别是只返回 $a、b$ 两个值。代码如下：

```Python
if __name__ == '__main__':
    file_name = "1-0-data.csv"
    samples = load_file(file_name)
    if samples is not None:
        X = samples[:, 0].reshape(200,1)
        Y = samples[:, 1].reshape(200,1)
        a, b = normal_equation(X,Y)
        print(str.format("a={0:.4f}, b={1:.4f}", a,b))
    else:
        print("cannot find file " + file_name)
```
得到结果：
```
a=0.5066, b=0.9921
```
结果和最小二乘的代数解法一致。


4 最小二乘法的局限性和适用场景
从上面可以看出，最小二乘法适用简洁高效，比梯度下降这样的迭代算法似乎方便很多，下面讨论最小二乘法的局限性。

首先，最小二乘法需要计算 [公式] 的逆矩阵，有可能它的逆矩阵不存在，这样就没有办法直接用最小二乘法了，此时梯度下降法仍然可以使用。可以通过对样本数据进行整理，去掉冗余特征，让 [公式] 的行列式不为0，然后继续使用最小二乘法。

第二，当样本特征 [公式] 非常大的时候，计算 [公式] 的逆矩阵是一个非常耗时的工作，甚至不可行。此时以梯度下降为代表的迭代法仍然可以使用。那这个n到底多大就不适合最小二乘法呢？如果你没有很多的分布式大数据计算资源，建议超过10000个特征就用迭代法吧。或者通过主成分分析降低特征的维度后再用最小二乘法。

第三，如果拟合函数不是线性的，这时无法使用最小二乘法，需要通过一些技巧转化为线性才能使用，此时梯度下降仍然可以用。

第四，讲一些特殊情况。当样本量 [公式] 很少，小于特征数 [公式] 的时候，这时拟合方程是欠定的，常用的优化方法都无法去拟合数据。当样本量 [公式] 等于特征数 [公式] 的时候，用方程组求解就可以了。当 [公式] 大于 [公式] 时，拟合方程是超定的，也就是我们常用与最小二乘法的场景了。



X1 = [1, 2, 3, 4]

X2 = [1, 3, 4, 3]

Y = [3.2, 4.1, 4.8, 5.1]

Y_hat1 = a1 * 1 + a2 * 1 + b

Y_hat2 = a1 * 2 + a2 * 3 + b

Y_hat3 = a1 * 3 + a2 * 4 + b

Y_hat4 = a1 * 4 + a2 * 3 + b

J = [3.2 - (a1+a2+b)]^2 + [4.1 - (2a1+3a2+b)]^2+ [4.8 - (3a1+4a2+b)]^2 + [5.1 - (4a1+3a2+b)]^2

dJ/da1 = -2(3.2-a1-a2-b)-4(4.1-2a1-3a2-b)-6(4.8-3a1-4a2-b)-8(5.1-4a1-3a2-b)=0

dJ/da2 = -2(3.2-a1-a2-b)-6(4.1-2a1-3a2-b)-8(4.8-3a1-4a2-b)-6(5.1-4a1-3a2-b)=0

dJ/db = -2(3.2-a1-a2-b)-2(4.1-2a1-3a2-b)-2(4.8-3a1-4a2-b)-2(5.1-4a1-3a2-b)=0

-6.4-16.4-28.8-40.8=-92.4
2a1+8a1+18a1+32a1=60a1
2a2+12a2+24a2+24a2=62a2
2b+4b+6b+8b=20b

c: -6.4-24.6-38.4-30.6=-100
a1: 2+12+24+24=62
a2: 2+18+32+18=70
b: 2+6+8+6=22

c: -6.4-8.2-9.6-10.2=-34.4
a1: 2+4+6+8=20
a2: 2+6+8+6=22
b: 2+2+2+2=8

60a1+62a2+20b=92.4
62a1+70a2+22b=100
20a1+22a2+8b=34.4

60a1+66a2+24b=103.2
4a2+4b=10.8
a2+b=2.7
a2 = 2.7-b

60a1+62(2.7-b)+20b=92.4
62a1+70(2.7-b)+22b=100

60a1+20b-62b=-75
62a1+22b-70b=-89

60a1-42b=-75
62a1-48b=-89

a1 = (42b-75)/60
62(42b-75)/60-48b=-89
43.4b-48b=-89+77.5
4.6b=11.5
b=2.5
a1=0.5
a2=0.2