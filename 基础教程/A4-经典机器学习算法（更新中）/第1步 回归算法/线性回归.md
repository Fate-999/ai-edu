
### 简单的直线拟合问题

大家在初中就学习过这样的问题：在平面上有两个点 $p_1(2,2)$ 和 $p_2(4,3)$，求解一条直线可以同时穿过这两个点。

<img src="./images/1-1.png" />

假设直线的方程为：

$$
y = ax + b \tag{1.1}
$$

那么就把问题转化为求解 $a$ 和 $b$ 的值，这是最简单的线性问题，只要把 $p_1$ 和 $p_2$ 的坐标值带入公式1.1，即可得到一个二元一次方程组：

$$
\begin{cases}
2=2a+b \\
3=4a+b
\end{cases}
$$

得到：$a=0.5，b=1$，则公式的实际形式为：$y=0.5x+1$，这样我们就得到了从两个样本点（$p_1,p_2$）拟合出来的直线。

### 一元线性回归模型

这是一个非常简单的理想化的数学问题，但是在客观世界中，由于受到各种因素的干扰，实际的公式可能是这样的：

$$
y=ax+b+\varepsilon \tag{1.2}
$$

公式1.2称为一元线性回归模型，$a$ 和 $b$ 是模型的参数，$x$ 是一元自变量，$y$ 是因变量。针对这一模型，有以下几个主要假定$^{[1]}$：

1. 因变量 $y$ 与自变量 $x$ 之间具有线性关系。
2. 在重复抽样中，自变量 $x$ 的取值是固定的，即假定 $x$ 是非随机的。
3. 误差项 $\varepsilon$ 是一个期望为 0 的随机变量，即 $E(\varepsilon)=0$。
4. 对于所有的 $x$ 值，$\varepsilon$ 的方差都相同。
5. 误差项 $\varepsilon$ 是一个服从正态分布的随机变量且独立，即 $\varepsilon \sim N(0,\sigma^2)$

上述假定意味着对于任何一个给定的 $x$，$y$ 的取值都对应着一个分布，所以 $y$ 的期望值 $E(y)=ax+b$，而 $y$ 的方差等于 $\varepsilon$ 的方差 $\sigma^2$。当 $\sigma^2$ 较小时，$y$ 的观测值非常靠近直线；当 $\sigma^2$ 较大时，$y$ 的观测值将偏离直线。

<img src="./images/1-2.png" />

从图1.2可以看到，$y$ 的期望值处于 $y=ax+b$ 直线之上，但是其实际的取值却是个正态分布。比如，$x1=2$ 时，$y$ 的取值可能是 $1.9, 1.93, 1.99, 2, 2.01, 2.02......$ 中的任何一个值。

### 制作数据集

根据公式1.2，下面我们来自己制作一个数据集，来探讨一元线性回归模型的解法。

先假设一个应用场景：在一个机房里，有 346 台计算机，需要空调来制冷，使得机房的温度保持在20度左右。显然空调的功率（千瓦/小时）与计算机的数量是相关的，而且是线性关系。需要解决的问题是找到这种线性关系，这个问题用定量的数学方法是很难解出来的，因为每台计算机的散热量不是一个定值，与其繁忙程度有关。所以一般会用现场实测的方式来解决。

针对公式1.2，假设 $a=0.5$，$b=1$，$\varepsilon \sim N(0,0.1)$，$x$（机房内的计算机数量）是一个从 0 到 1000 的随机数，一共有需要 100 个机房的样本数据，于是有代码如下：

```Python
a = 0.5         # 参数a
b = 1           # 参数b
m = 100         # 模拟100个机房的样本
def generate_samples_1(a, b, m):
    # m个[0,1)之间随机数，表示机房内计算机数量/1000
    X = np.random.random(size=(m, 1))
    # 返回均值为0，方差为0.1的误差的一组值
    Epsilon = np.random.normal(loc=0, scale=0.1, size=X.shape)
    Y = a * X + b + Epsilon
    return X,Y
```

且慢！上面的代码并不正确，在生成 Epsilon 时，只生成了一组（100个）正态分布的噪音数据，然后就在最后一行代码通过矩阵相加的形式，顺序地分配给了 $X$。这就相当于不同的 $x$ 取值，其所有的 $y$ 值整体的噪音是一个正态分布，而不是基于每个 $y$ 的独立的正态分布。

所以，我们改变代码如下：
```Python
def generate_samples_2(a, b, m):
    # m个[0,1)之间随机数，表示机房内计算机数量/1000
    X = np.random.random(size=(m, 1))
    Y = np.zeros_like(X)
    for i in range(m):
        # 返回均值为0，方差为0.1的误差的一个值
        epsilon = np.random.normal(loc=0, scale=0.1, size=None)
        # 对于每个特定的x值，都从N(0,0.1)中取出一个随机值作为噪音添加到y上
        Y[i,0] = a * X[i,0] + b + epsilon
    return X,Y
```

在生成数据后，我们用 matplotlib 库来做一下可视化，得到图1.3。

<img src="./images/1-3.png" />

在图1.3中，每个样本点代表某个机房的情况：
- 横坐标是计算机的数量，用“千台”作为单位；
- 纵坐标是需要空调的功率，用“千瓦”作为单位。

### 最小二乘法（Method of least squares）

有了样本数据后，我们通过可视化可以看到这个问题应该可以通过线性回归的问题来解决。如果在高维空间没有可视化手段时，一般也是先用简单的线性回归来尝试解决，如果效果不令人满意的话，再尝试非线性的模型。

在图1.3中，我们发现有很多条直线可以穿过整个样本区来拟合，它们的 $a, b$ 值各不相同，哪一条是最佳的呢？

德国科学家卡尔·高斯（Karl Gauss，1777-1855）提出用图1.4中竖直方向的距离 Error 的平方和来估计参数 $a$ 和 $b$，该方法称为最小二乘法或最小平方法。

<img src="./images/1-4.png" />

用最小二乘法拟合的直线具有一些优良的性质$^{[1]}$：

1. 该方法得到的回归直线能使离差平方和最小，虽然这并不能保证它就是拟合数据的最佳直线，但也是足够好的。
2. 该方法可以得知 $a$ 和 $b$ 的估计量的抽样分布。
3. 在某些条件下，与其他方法相比，其抽样分布具有较小的标准差。

在图1.4中，拟合的直线方程是：

$$
\hat y = \hat a x + \hat b
$$

$$
error = y - \hat{y}=y -  \hat a x + \hat b\\
$$

令$E(a,b))=\sum error^2$

$$
error^2 = (y - \hat{y})^2 = (y -  \hat a x + \hat b)^2 \\
\sum_{i=1}^n error^2 = \sum_{i=1}^n (y -  \hat a x + \hat b)^2 
$$


